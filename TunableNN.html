

<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Tunable Neural Networks for CT Image Formation &#8212; Statistical Methods for Spectral CT Imaging and Material Decomposition</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=e353d410970836974a52" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=e353d410970836974a52" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=e353d410970836974a52" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=e353d410970836974a52" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52" />

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'TunableNN';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="Introduction.html">
  
  
  
  
    
    
      
    
    
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image"/>
    <script>document.write(`<img src="_static/logo.png" class="logo__image only-dark" alt="Logo image"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Introduction.html">
                    <no title>
                </a>
            </li>
        </ul>
        <ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="tivnan2019physical.html">INTRODUCTION {#sec:intro}</a></li>



<li class="toctree-l1"><a class="reference internal" href="tivnan2019optimized.html">Filter Tile Width</a></li>

</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/tivnanmatt/dissertation" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/tivnanmatt/dissertation/issues/new?title=Issue%20on%20page%20%2FTunableNN.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/TunableNN.md" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Tunable Neural Networks for CT Image Formation</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract">ABSTRACT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">INTRODUCTION</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#methods">METHODS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-model-of-ct-data-acquisition-and-image-formation">Probabilistic Model of CT Data Acquisition and Image Formation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-and-bias-in-reconstructed-images">Variance and Bias in Reconstructed Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean">Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#patient-population-model">Patient Population Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#physical-measurement-model">Physical Measurement Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-filtered-back-projection">Image Reconstruction via Filtered Back-Projection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-model-based-iterative-statistical-estimation">Image Reconstruction via Model-Based Iterative Statistical Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-deep-neural-networks">Image Reconstruction via Deep Neural Networks</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="tunable-neural-networks-for-ct-image-formation">
<h1>Tunable Neural Networks for CT Image Formation<a class="headerlink" href="#tunable-neural-networks-for-ct-image-formation" title="Permalink to this heading">#</a></h1>
<section id="abstract">
<h2>ABSTRACT<a class="headerlink" href="#abstract" title="Permalink to this heading">#</a></h2>
<p>Optimization of CT image quality typically involves balancing variance and bias. In traditional filtered back-projection, this trade-off is controlled by the filter cutoff frequency. In model-based iterative reconstruction, the regularization strength parameter often serves the same function. Deep neural networks (DNNs) typically do not provide this tunable control over output image properties. Models are often trained to minimize the expected mean squared error, which penalizes both variance and bias in image outputs, but does not offer any control over the trade-off between the two. In this work, we propose a method for controlling the output image properties of neural networks with a new loss function called weighted covariance and bias (WCB). Our proposed method uses multiple noise realizations of the input images during training to allow for separate weighting matrices for the variance and bias penalty terms. Moreover, we show that tuning these weights enables targeted penalization of specific image features with spatial frequency domain penalties. To evaluate our method, we present a simulation study using digital anthropomorphic phantoms, physical simulation of CT measurements, and image formation with various algorithms. We show that the WCB loss function offers a greater degree of control over trade-offs between variance and bias, while MSE provides only one specific image quality configuration. We also show that WCB can be used to control specific image properties including variance, bias, spatial resolution, and the noise correlation of neural network outputs. Finally, we present a method to optimize the proposed weights for a spiculated lung nodule shape discrimination task. Our results demonstrate this new image quality can control the image properties of DNN outputs and optimize image quality for task-specific performance.</p>
</section>
<section id="introduction">
<h2>INTRODUCTION<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
<p>Medical CT systems use the transmission of high-energy x-ray photons from multiple view angles to reconstruct the spatial distribution of an object’s attenuation properties. The resulting images are used by clinicians to inform clinical decisions, such as diagnosis, surgical guidance, or therapy planning. Ideally, CT images should be both precise and accurate to minimize errors in clinical decision making. In this work, we use random signal analysis to model the propagation of signals, noise, and bias in CT data processing, where precision is characterized by the variance of random variables, and accuracy is characterized by their bias.</p>
<p>The photon count in CT data processing is typically modeled as a Poisson-distributed random variable with a single parameter, <span class="math notranslate nohighlight">\(\lambda\)</span>, which is both the mean and variance, due to practical engineering limitations that do not allow most x-ray sources to control the exact number of photons transmitted. Since the photon count is typically above one thousand, even for low-dose CT, the distribution is well approximated by a Gaussian-distributed random variable with mean <span class="math notranslate nohighlight">\(\mu = \lambda\)</span> and variance <span class="math notranslate nohighlight">\(\sigma^2 = \lambda\)</span>. This quantum noise is one physical source of uncertainty leading to noise in CT measurements.</p>
<p>A full CT dataset can be expressed as a list of random variables or a random vector, where the elements of this vector may or may not be independently distributed depending on the type of detector and other factors. For example, modeling measurements as jointly Gaussian is a reasonable assumption for a flat-panel indirect detector, where the measurements of neighboring pixels are correlated due to light spreading in the scintillator layer.</p>
<p>CT is a computational imaging modality, meaning the measured data must be transformed into images via a reconstruction algorithm. In this work, we focus on deterministic image reconstruction algorithms, where a given set of input CT measurements always maps to the same output image. However, we do not assume that this functional mapping is linear for all cases, only that it is deterministic. Reconstruction algorithms can have different trade-offs between accuracy and precision. Some algorithms, such as ramp-filtered back-projection (FBP) or maximum-likelihood model-based iterative reconstruction (ML-MBIR), produce images with high accuracy but low precision, while others, such as FBP with an apodization filter or penalized-likelihood MBIR (PL-MBIR), reduce variance at the cost of some bias in the form of spatial blur.</p>
<p>Recent years have seen the introduction of machine learning methods, including deep neural networks (DNNs), for CT image formation such as reconstruction, denoising, and restoration. These methods produce highly favorable images by many quality measures, and the trade-off between variance and bias in output images is modulated by the contents of the training dataset, the choice of loss function, and other network training techniques. Different loss functions can be used for DNN training, such as mean-squared error (MSE) with respect to some supervised training target images. However, MSE does not provide any parametric weighting on different types of errors, so the relative emphasis on reducing variance or bias cannot be controlled by an imaging algorithm developer. Recently, several new deep learning training strategies have been proposed to improve CT image quality beyond MSE, such as by including a mathematical model of perceptual similarity or a discriminator loss term in the training loss function or using two noise realizations during training to reduce bias. We believe that high-quality CT images are those which optimize a trade-off between precision and accuracy according to the clinical requirements.</p>
</section>
<section id="methods">
<h2>METHODS<a class="headerlink" href="#methods" title="Permalink to this heading">#</a></h2>
<section id="probabilistic-model-of-ct-data-acquisition-and-image-formation">
<h3>Probabilistic Model of CT Data Acquisition and Image Formation<a class="headerlink" href="#probabilistic-model-of-ct-data-acquisition-and-image-formation" title="Permalink to this heading">#</a></h3>
<p>Consider a medical imaging scenario where a patient, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, undergoes a CT scan to collect noisy projection-domain measurements, <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, which are processed to produce a reconstructed image of the patient, <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span>. We define the vector space, <span class="math notranslate nohighlight">\(\boldsymbol{\mathcal{X}} = \Re^{(N_\text{voxel} \times 1)}\)</span>, the domain of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span>, to be a lexicographic column vector representation of a three-dimensional voxelized image volume of linear attenuation coefficients. We define the vector space, <span class="math notranslate nohighlight">\(\boldsymbol{\mathcal{Y}} = \Re^{(N_\text{projection} \times 1)}\)</span>, the domain of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>, to be a column vector representing projection-domain x-ray photon counts for each detector pixel and each view angle in the CT data acquisition.
The patient, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, is one sample from a larger patient population consisting of a wide range of shapes, sizes, and anatomical features. Also, the measurements <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> are affected by quantum noise and so will not be the same for repeated data acquisitions. For this reason, we model <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> as random vectors. Also, we assume that the reconstructed image, <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span>, is a deterministic function of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> and is, therefore, a random vector as well. Our probabilistic model of CT image formation is given by the joint probability density function,</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x}, \mathbf{y}, \mathbf{\hat{x}}) = p(\mathbf{x})p(\mathbf{y}|\mathbf{x})p(\mathbf{\hat{x}}|\mathbf{y})\]</div>
<p>Notice that the joint distribution can be broken into three parts. The term <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> represents the patient population, <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span> is the physical model of data acquisition including noise, and <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{y})\)</span> represents image reconstruction, which we assume is a deterministic discrete-to-discrete transformation, <span class="math notranslate nohighlight">\(f_\theta: \boldsymbol{\mathcal{Y}} \longrightarrow \boldsymbol{\mathcal{X}}\)</span>. That is, <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{y}) = \delta(\mathbf{\hat{x}} - f_{\theta}(\mathbf{y}))\)</span>, where  <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span> is a vector of reconstruction hyperparameters (e.g., FBP cutoff frequency or DNN weights). Note that we have also used the property <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{y}, \mathbf{x})\)</span> = <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{y})\)</span>. Since the reconstruction is a deterministic function of the measurements, we know that <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span> is conditionally independent of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, if given <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span>. We aim to apply this probabilistic model to understand and optimize the quality of images to improve expected health outcomes for patients.</p>
<p><img alt="Probabilistic graphical model of CT data acquisition and image formation." src="_images/probModel.png" /></p>
</section>
<section id="variance-and-bias-in-reconstructed-images">
<h3>Variance and Bias in Reconstructed Images<a class="headerlink" href="#variance-and-bias-in-reconstructed-images" title="Permalink to this heading">#</a></h3>
<p>The joint distribution of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span> can be calculated by marginalizing over <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> as follows:</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{x},\mathbf{\hat{x}}) = \int p(\mathbf{x},\mathbf{y},\mathbf{\hat{x}}) \mathbf{dy} = p(\mathbf{x}) \Big[ \int p(\mathbf{y}|\mathbf{x})p(\mathbf{\hat{x}}|\mathbf{y}) \mathbf{dy}\Big] = p(\mathbf{x}) p(\mathbf{\hat{x}} | \mathbf{x}) .\]</div>
<p>Note that this distribution is broken into two parts; <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span> is the patient population model, and <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{x})\)</span> is the end-to-end CT imaging system model, which captures both data acquisition and image reconstruction. Image quality metrics such as noise or spatial resolution are (typically patient-dependent) properties of an imaging system which can be derived from <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{x})\)</span>.</p>
<section id="mean">
<h4>Mean<a class="headerlink" href="#mean" title="Permalink to this heading">#</a></h4>
<p>For example, the mean response of the imaging system is given by:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}} = \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[\mathbf{\hat{X}}|\mathbf{x}\Big]  = \int  p(\mathbf{\hat{x}}|\mathbf{x}) \enspace \mathbf{\hat{x}} \enspace \mathbf{d\hat{x}} = \int  \Big[\int  p(\mathbf{y}|\mathbf{x}) \delta(\mathbf{\hat{x}} - f_{\theta} ( \mathbf{y} )) \mathbf{dy}\Big] \enspace \mathbf{\hat{x}} \enspace \mathbf{d\hat{x}} \]</div>
<div class="math notranslate nohighlight">
\[= \int p(\mathbf{y}|\mathbf{x}) \Big[\int   \delta(\mathbf{\hat{x}} - f_{\theta} ( \mathbf{y} )) \enspace \mathbf{\hat{x}} \enspace \mathbf{d\hat{x}} \Big]  \mathbf{dy}  = \int p(\mathbf{y}|\mathbf{x}) \enspace f_{\theta} ( \mathbf{y} ) \enspace \mathbf{dy}.\]</div>
</section>
<section id="bias">
<h4>Bias<a class="headerlink" href="#bias" title="Permalink to this heading">#</a></h4>
<p>The corresponding bias (defined as the expected error) is given by:</p>
<div class="math notranslate nohighlight">
\[\mathbf{b}_\mathbf{\hat{x}|x} =\mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[(\mathbf{\hat{X}}-\mathbf{x})|\mathbf{x}\Big]  = \int p(\mathbf{y}|\mathbf{x}) \Big[f_{\theta} ( \mathbf{y} )-\mathbf{x} \Big] \mathbf{dy} =  \boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}} - \mathbf{x}.\]</div>
</section>
<section id="covariance">
<h4>Covariance<a class="headerlink" href="#covariance" title="Permalink to this heading">#</a></h4>
<p>The covariance, which describes noise properties, is given by:</p>
<div class="math notranslate nohighlight">
\[\boldsymbol{\Sigma}_{\mathbf{\hat{x}}|\mathbf{x}} = \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[(\mathbf{\hat{X}} - \boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}})(\mathbf{\hat{X}} - \boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}})^T |\mathbf{x}\Big] = \int p(\mathbf{y}|\mathbf{x}) \enspace      \Big[ (f_{\theta} ( \mathbf{y} ) - \boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}})(f_{\theta} ( \mathbf{y} ) - \boldsymbol{\mu}_{\mathbf{\hat{x}}|\mathbf{x}})^T  \Big]     \enspace \mathbf{dy}.\]</div>
<p>These expressions for bias and covariance describe two of the most important categories of image error. An imaging system with a high level of bias, <span class="math notranslate nohighlight">\(\mathbf{b}_\mathbf{\hat{x}|x}\)</span>, is inaccurate on average. Note that this includes all forms of mean error, not only zero-frequency offset, as the term ``bias’’ is sometimes used. Common forms of bias include spatial blur and artifacts due to miscalibration or other mismatches between the model used for data processing and the true physical object. An imaging system with a higher covariance, <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}_{\mathbf{\hat{x}}|\mathbf{x}}\)</span> is imprecise and will produce noisier images which can make it harder to see clinically important features. The main topic of this work is a method to control the trade-off between variance and bias in reconstructed images in order to avoid potentially severe errors that negatively impact clinical decision-making. Samples from these distributions as well as the mean, bias, and standard deviation of output are shown in Figure~\ref{fig:biasVariance_example} to illustrate these mathematical terms. This example shows some common types of error, such as higher bias at edges, which corresponds to lower spatial resolution. The standard deviation map also shows higher noise inside the patient vs in the air regions, which may indicate the network has learned to implicitly identify air regions and remove almost all noise.</p>
<p><img alt="probablity_distribution_samples" src="_images/probablity_distribution_samples.png" /></p>
</section>
</section>
<section id="patient-population-model">
<h3>Patient Population Model<a class="headerlink" href="#patient-population-model" title="Permalink to this heading">#</a></h3>
<p>The marginal distribution, <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>, models the population of patients. In practice, we do not have access to this distribution, but we can approximate expectations over this distribution using a dataset of patient images and assuming they are independent samples from <span class="math notranslate nohighlight">\(p(\mathbf{x})\)</span>. Using a database of CT images is a very common strategy for data-driven methods such as deep learning \cite{wang2020deep}. However, with this approach, the reference images are reconstructed CT images rather than true patient anatomies. These reconstructed images have noise and bias (e.g., blur, artifacts, etc.) and if they are used as the ground-truth supervised labels, the DNN will be trained to replicate the same sort of errors in the training images.</p>
<p>As an alternative, we propose another method using a population of digital anthropomorphic phantoms, such as XCAT \cite{segars20104d}. The XCAT software takes a list of <span class="math notranslate nohighlight">\(N_\text{anatomy}\)</span> anatomical parameters (e.g., liver volume, trachea thickness) which we can collect into a vector space, <span class="math notranslate nohighlight">\(\boldsymbol{\mathcal{Z}} = \Re^{(N_\text{anatomy}\times 1)}\)</span>. The XCAT phantom generator is then represented by a deterministic discrete-to-discrete transformation <span class="math notranslate nohighlight">\(\boldsymbol{\mathcal{G}}:\boldsymbol{\mathcal{Z}}\longrightarrow \boldsymbol{\mathcal{X}}\)</span>. We model the anatomical parameters as a Gaussian distributed random vector <span class="math notranslate nohighlight">\(\mathbf{Z}\)</span> with mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu_z}\)</span> and covariance <span class="math notranslate nohighlight">\(D\{\boldsymbol{\sigma_z^2}\}\)</span>, where the <span class="math notranslate nohighlight">\(D\{\cdot\}\)</span> notation indicates the diagonalization of a vector. Those parameters are passed to the XCAT generator to create a population of ground-truth patient images with varied anatomies. This way, we can compute population-level statistics using sample expectations over these images. While XCAT phantoms are not entirely realistic in terms of texture and the full range of anatomical variations, diseases, and disorders, these anatomical models are useful. In particular, this methodology is illustrative of the overall technique and provides access to the ground-truth attenuation, which we will use to evaluate bias in reconstructed images. In the future, high-fidelity clinical images or more sophisticated anatomical models could be used to model the ground truth patient population more accurately.</p>
</section>
<section id="physical-measurement-model">
<h3>Physical Measurement Model<a class="headerlink" href="#physical-measurement-model" title="Permalink to this heading">#</a></h3>
<p>The measurement likelihood, <span class="math notranslate nohighlight">\(p(\mathbf{y}|\mathbf{x})\)</span>, represents the physical model of CT data acquisition including noise. For this work, we will use the following nonlinear mono-energetic forward model with uncorrelated Gaussian noise as follows:</p>
<div class="math notranslate nohighlight">
\[p(\mathbf{y}|\mathbf{x}) = \mathcal{N}\Big(\mathbf{y}; \boldsymbol{\mu_y}(\mathbf{x}), \boldsymbol{\Sigma_y}(\mathbf{x}) \Big) \quad , \quad \boldsymbol{\mu_y}(\mathbf{x}) = \mathbf{B} \exp{\Big(-\mathbf{A} \mathbf{x}\Big)} \quad , \quad \boldsymbol{\Sigma_y}(\mathbf{x}) = D\Big\{\boldsymbol{\mu_y}(\mathbf{x})\Big\}  .\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is the forward projector describing line integral sampling geometry and <span class="math notranslate nohighlight">\(\mathbf{B}\)</span> is a projection-domain linear operator which can model gain and blur effects. The exponential operation is element-wise. The variance is set equal to the mean model, which is designed to approximate a Poisson distribution associated with quantum noise. Again, more sophisticated models may be adopted and applied using the same framework.</p>
</section>
<section id="image-reconstruction-via-filtered-back-projection">
<h3>Image Reconstruction via Filtered Back-Projection<a class="headerlink" href="#image-reconstruction-via-filtered-back-projection" title="Permalink to this heading">#</a></h3>
<p>The third term in equation \eqref{eq:joint_pdf} is <span class="math notranslate nohighlight">\(p(\mathbf{\hat{x}}|\mathbf{y}) = \delta(\mathbf{\hat{x}} - f_{\theta} ( \mathbf{y} ))\)</span>, which represents image reconstruction. A common approach to reconstruct CT images is filtered back-projection with an apodization filter, which can be written as:</p>
<div class="math notranslate nohighlight">
\[f_\sigma(\mathbf{y}) =  \mathbf{A^T} \boldsymbol{P}_\sigma [ -\log \frac{\mathbf{y}}{\boldsymbol{\mu_y(0)}}],\]</div>
<p>where <span class="math notranslate nohighlight">\(\boldsymbol{P}_\sigma\)</span> is a projection-domain linear operator consisting of, for example, projection weighting, ramp filtration, and an apodization filter. <span class="math notranslate nohighlight">\(\mathbf{A^T}\)</span> is the back-projector, which is the transpose of the forward-projector <span class="math notranslate nohighlight">\(\mathbf{A}\)</span>. We use a Gaussian apodization filter, with the hyperparameter <span class="math notranslate nohighlight">\(\sigma\)</span> representing spatial-domain full width at half maximum.  We will use this classical image reconstruction algorithm as a reference to evaluate our proposed method.</p>
</section>
</section>
<section id="image-reconstruction-via-model-based-iterative-statistical-estimation">
<h2>Image Reconstruction via Model-Based Iterative Statistical Estimation<a class="headerlink" href="#image-reconstruction-via-model-based-iterative-statistical-estimation" title="Permalink to this heading">#</a></h2>
<p>Another approach is to perform image reconstruction via a statistical estimation algorithm such as penalized-likelihood model-based iterative reconstruction (PL-MBIR) which is given by</p>
<div class="math notranslate nohighlight">
\[
    f_\beta(\mathbf{y}) = \underset{\mathbf{x}}{\text{argmin}} \quad \frac{1}{2} (\mathbf{y} - \boldsymbol{\mu_y}(\mathbf{x}))^T \boldsymbol{\Sigma_y}^{-1} (\mathbf{y} - \boldsymbol{\mu_y}(\mathbf{x})) + \beta \mathbf{x}^T \boldsymbol{\nabla^2} \mathbf{x} .
\]</div>
<p>Here, the first term is derived from the negative logarithm of \eqref{eq:likelihood} and encourages agreement between the image reconstruction and the measurement data, and the second term is a quadratic roughness penalty which encourages smoothness (and reduces noise). The operator, <span class="math notranslate nohighlight">\(\boldsymbol{\nabla^2}\)</span>, is the discrete Laplacian, which is a type of high-pass filter and serves to put a penalty weight on higher spatial frequencies where  higher noise is predicted by the model. We optimize this nonlinear weighted least squares objective function using the algorithm described in \cite{tilley2015model}.</p>
<p>The hyperparameter <span class="math notranslate nohighlight">\(\beta\)</span> can be viewed as a tunable dial which controls the trade-off between noise and spatial blur. This way, it is possible to have different protocols for applications with greater emphasis on either noise reduction or spatial resolution. Inspired by this general structure, we aim  to provide a similar method of tunable control over output image qualities for deep learning based image formation models.</p>
</section>
<section id="image-reconstruction-via-deep-neural-networks">
<h2>Image Reconstruction via Deep Neural Networks<a class="headerlink" href="#image-reconstruction-via-deep-neural-networks" title="Permalink to this heading">#</a></h2>
<p>Another approach to CT image formation is to train a DNN for image reconstruction, restoration, or denoising. DNN models have numerous network weight parameters, <span class="math notranslate nohighlight">\(\boldsymbol{\theta}\)</span>, which must be trained to minimize some loss function using large datasets of CT images. One of the most commonly used loss functions for deep learning image formation is mean-squared error (MSE) for which the network training process can be written as</p>
<div class="math notranslate nohighlight">
\[
     \boldsymbol{\hat{\theta}} = \underset{\boldsymbol{\theta}}{\text{argmin}} \enspace \mathbb{E}_\mathbf{\mathbf{X},\hat{X}}\Big[||\mathbf{\hat{X}}-\mathbf{X}||^2\Big]  = \mathbb{E}_{\mathbf{X}}\Big[\mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[ (\mathbf{\hat{X}}-\mathbf{x})^T(\mathbf{\hat{X}} - \mathbf{x}) | \mathbf{X}\Big]\Big] 
\]</div>
<p>The inner term is the conditional expectation, <span class="math notranslate nohighlight">\(\mathbb{E}_\mathbf{\hat{X}|\mathbf{X}}[(\mathbf{\hat{X}}-\mathbf{x})^T(\mathbf{\hat{X}}-\mathbf{x})|\mathbf{x}]\)</span> which can be expanded and written as a function of bias from \eqref{eq:bias} and covariance from \eqref{eq:covariance} as follows:</p>
<div class="math notranslate nohighlight">
\[
    \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[(\mathbf{\hat{X}}-\mathbf{x})^T(\mathbf{\hat{X}}-\mathbf{x})|\mathbf{x}\Big] = \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[(\mathbf{\hat{X}}- \boldsymbol{\mu}_\mathbf{\hat{x}|x} + \mathbf{b}_\mathbf{\hat{x}|x})^T(\mathbf{\hat{X}}-\boldsymbol{\mu}_\mathbf{\hat{x}|x} + \mathbf{b}_\mathbf{\hat{x}|x})|\mathbf{x}\Big] 
\]</div>
<div class="math notranslate nohighlight">
\[
    = \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[(\mathbf{\hat{X}}- \boldsymbol{\mu}_\mathbf{\hat{x}|x} )^T(\mathbf{\hat{X}}-\boldsymbol{\mu}_\mathbf{\hat{x}|x})|\mathbf{x}\Big]  + \mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}\Big[( \mathbf{b}_\mathbf{\hat{x}|x})^T(\mathbf{b}_\mathbf{\hat{x}|x})|\mathbf{x}\Big]  = \text{Tr}\Big\{\boldsymbol{\Sigma}_\mathbf{\hat{x}|x}\Big\} +  \mathbf{b}_\mathbf{\hat{x}|x}^T \mathbf{b}_\mathbf{\hat{x}|x}. \nonumber
\]</div>
<p>Note that this formula shows that MSE can be decomposed into the expected sum, <span class="math notranslate nohighlight">\(\text{Tr}\Big\{\boldsymbol{\Sigma}_\mathbf{\hat{x}|x}\Big\}\)</span>, which is a sum of variances for each voxel, and <span class="math notranslate nohighlight">\(\mathbf{b}_\mathbf{\hat{x}|x}^T \mathbf{b}_\mathbf{\hat{x}|x}\)</span>, which is the sum of squared biases. Therefore, training a network to minimize MSE can be written as</p>
<div class="math notranslate nohighlight">
\[
    \boldsymbol{\hat{\theta}} = \underset{\boldsymbol{\theta}}{\text{argmin}} \quad \mathbb{E}_\mathbf{X}\Big[\text{Tr}\Big\{\boldsymbol{\Sigma}_\mathbf{\hat{x}|x}\Big\} +  \mathbf{b}_\mathbf{\hat{x}|x}^T \mathbf{b}_\mathbf{\hat{x}|x}\Big].
\]</div>
<p>In practice, the minimizer is implemented via numerical optimization methods such as stochastic gradient descent. The expectation over <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is approximated by sampling a batch of patient images and computing the sample mean over the batch. Typically, for a given image in the database, <span class="math notranslate nohighlight">\(\mathbf{X}=\mathbf{x_i}\)</span>, there is only one noise realization of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> and so during training, only one realization of the output, <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}\)</span>, is available. Therefore, the expectation, <span class="math notranslate nohighlight">\(\mathbb{E}_{\mathbf{\hat{X}}|\mathbf{X}}[(\mathbf{\hat{X}}-\mathbf{x})^T(\mathbf{\hat{X}}-\mathbf{x})|\mathbf{x}]\)</span>, must be replaced by the squared error for one sample <span class="math notranslate nohighlight">\((\mathbf{\hat{x_i}}-\mathbf{x_i})^T(\mathbf{\hat{x}_i}-\mathbf{x_i})\)</span>. On the other hand, if multiple noise realizations of <span class="math notranslate nohighlight">\(\mathbf{Y}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\hat{X}}=f_{\theta} ( \mathbf{y} )\)</span> were available, it would be possible to directly apply \eqref{eq:bias} and \eqref{eq:covariance} to compute the sample bias and covariance as well as the optimization in \eqref{eq:MSE_training}. The next section presents a training scheme which uses multiple noise realizations to explicitly compute and control the variance and bias of DNN output images.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
                <footer class="bd-footer-article">
                  
<div class="footer-article-items footer-article__inner">
  
    <div class="footer-article-item"><!-- Previous / next buttons -->
<div class="prev-next-area">
</div></div>
  
</div>

                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#abstract">ABSTRACT</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">INTRODUCTION</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#methods">METHODS</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#probabilistic-model-of-ct-data-acquisition-and-image-formation">Probabilistic Model of CT Data Acquisition and Image Formation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#variance-and-bias-in-reconstructed-images">Variance and Bias in Reconstructed Images</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mean">Mean</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#bias">Bias</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#covariance">Covariance</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#patient-population-model">Patient Population Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#physical-measurement-model">Physical Measurement Model</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-filtered-back-projection">Image Reconstruction via Filtered Back-Projection</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-model-based-iterative-statistical-estimation">Image Reconstruction via Model-Based Iterative Statistical Estimation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#image-reconstruction-via-deep-neural-networks">Image Reconstruction via Deep Neural Networks</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Matthew Tivnan
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=e353d410970836974a52"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=e353d410970836974a52"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>