# Tunable Neural Networks for CT Image Formation

## Abstract

Optimization of CT image quality typically involves balancing variance and bias. In traditional filtered back-projection, this trade-off is controlled by the filter cutoff frequency. In model-based iterative reconstruction, the regularization strength parameter often serves the same function. Deep neural networks (DNNs) typically do not provide this tunable control over output image properties. Models are often trained to minimize the expected mean squared error, which penalizes both variance and bias in image outputs, but does not offer any control over the trade-off between the two. In this work, we propose a method for controlling the output image properties of neural networks with a new loss function called weighted covariance and bias (WCB). Our proposed method uses multiple noise realizations of the input images during training to allow for separate weighting matrices for the variance and bias penalty terms. Moreover, we show that tuning these weights enables targeted penalization of specific image features with spatial frequency domain penalties. To evaluate our method, we present a simulation study using digital anthropomorphic phantoms, physical simulation of CT measurements, and image formation with various algorithms. We show that the WCB loss function offers a greater degree of control over trade-offs between variance and bias, while MSE provides only one specific image quality configuration. We also show that WCB can be used to control specific image properties including variance, bias, spatial resolution, and the noise correlation of neural network outputs. Finally, we present a method to optimize the proposed weights for a spiculated lung nodule shape discrimination task. Our results demonstrate this new image quality can control the image properties of DNN outputs and optimize image quality for task-specific performance.

## Introduction

Medical CT systems use the transmission of high-energy x-ray photons from multiple view angles to reconstruct the spatial distribution of an object's attenuation properties. The resulting images are used by clinicians to inform clinical decisions, such as diagnosis, surgical guidance, or therapy planning. Ideally, CT images should be both precise and accurate to minimize errors in clinical decision making. In this work, we use random signal analysis to model the propagation of signals, noise, and bias in CT data processing, where precision is characterized by the variance of random variables, and accuracy is characterized by their bias.

The photon count in CT data processing is typically modeled as a Poisson-distributed random variable with a single parameter, $\lambda$, which is both the mean and variance, due to practical engineering limitations that do not allow most x-ray sources to control the exact number of photons transmitted. Since the photon count is typically above one thousand, even for low-dose CT, the distribution is well approximated by a Gaussian-distributed random variable with mean $\mu = \lambda$ and variance $\sigma^2 = \lambda$. This quantum noise is one physical source of uncertainty leading to noise in CT measurements.

A full CT dataset can be expressed as a list of random variables or a random vector, where the elements of this vector may or may not be independently distributed depending on the type of detector and other factors. For example, modeling measurements as jointly Gaussian is a reasonable assumption for a flat-panel indirect detector, where the measurements of neighboring pixels are correlated due to light spreading in the scintillator layer.

CT is a computational imaging modality, meaning the measured data must be transformed into images via a reconstruction algorithm. In this work, we focus on deterministic image reconstruction algorithms, where a given set of input CT measurements always maps to the same output image. However, we do not assume that this functional mapping is linear for all cases, only that it is deterministic. Reconstruction algorithms can have different trade-offs between accuracy and precision. Some algorithms, such as ramp-filtered back-projection (FBP) or maximum-likelihood model-based iterative reconstruction (ML-MBIR), produce images with high accuracy but low precision, while others, such as FBP with an apodization filter or penalized-likelihood MBIR (PL-MBIR), reduce variance at the cost of some bias in the form of spatial blur. 

Recent years have seen the introduction of machine learning methods, including deep neural networks (DNNs), for CT image formation such as reconstruction, denoising, and restoration. These methods produce highly favorable images by many quality measures, and the trade-off between variance and bias in output images is modulated by the contents of the training dataset, the choice of loss function, and other network training techniques. Different loss functions can be used for DNN training, such as mean-squared error (MSE) with respect to some supervised training target images. However, MSE does not provide any parametric weighting on different types of errors, so the relative emphasis on reducing variance or bias cannot be controlled by an imaging algorithm developer. Recently, several new deep learning training strategies have been proposed to improve CT image quality beyond MSE, such as by including a mathematical model of perceptual similarity or a discriminator loss term in the training loss function or using two noise realizations during training to reduce bias. We believe that high-quality CT images are those which optimize a trade-off between precision and accuracy according to the clinical requirements.


## METHODS


### Probabilistic Model of CT Data Acquisition and Image Formation

Consider a medical imaging scenario where a patient, $\mathbf{X}$, undergoes a CT scan to collect noisy projection-domain measurements, $\mathbf{Y}$, which are processed to produce a reconstructed image of the patient, $\mathbf{\hat{X}}$. We define the vector space, $\boldsymbol{\mathcal{X}} = \Re^{(N_\text{voxel} \times 1)}$, the domain of $\mathbf{X}$ and $\mathbf{\hat{X}}$, to be a lexicographic column vector representation of a three-dimensional voxelized image volume of linear attenuation coefficients. We define the vector space, $\boldsymbol{\mathcal{Y}} = \Re^{(N_\text{projection} \times 1)}$, the domain of $\mathbf{Y}$, to be a column vector representing projection-domain x-ray photon counts for each detector pixel and each view angle in the CT data acquisition. 
The patient, $\mathbf{X}$, is one sample from a larger patient population consisting of a wide range of shapes, sizes, and anatomical features. Also, the measurements $\mathbf{Y}$ are affected by quantum noise and so will not be the same for repeated data acquisitions. For this reason, we model $\mathbf{X}$ and $\mathbf{Y}$ as random vectors. Also, we assume that the reconstructed image, $\mathbf{\hat{X}}$, is a deterministic function of $\mathbf{Y}$ and is, therefore, a random vector as well. Our probabilistic model of CT image formation is given by the joint probability density function,

$$p(\mathbf{x}, \mathbf{y}, \mathbf{\hat{x}}) = p(\mathbf{x})p(\mathbf{y}|\mathbf{x})p(\mathbf{\hat{x}}|\mathbf{y})$$

Notice that the joint distribution can be broken into three parts. The term $p(\mathbf{x})$ represents the patient population, $p(\mathbf{y}|\mathbf{x})$ is the physical model of data acquisition including noise, and $p(\mathbf{\hat{x}}|\mathbf{y})$ represents image reconstruction, which we assume is a deterministic discrete-to-discrete transformation, $f_\theta: \boldsymbol{\mathcal{Y}} \longrightarrow \boldsymbol{\mathcal{X}}$. That is, $p(\mathbf{\hat{x}}|\mathbf{y}) = \delta(\mathbf{\hat{x}} - f_{\theta}(\mathbf{y}))$, where  $\boldsymbol{\theta}$ is a vector of reconstruction hyperparameters (e.g., FBP cutoff frequency or DNN weights). Note that we have also used the property $p(\mathbf{\hat{x}}|\mathbf{y}, \mathbf{x})$ = $p(\mathbf{\hat{x}}|\mathbf{y})$. Since the reconstruction is a deterministic function of the measurements, we know that $\mathbf{\hat{X}}$ is conditionally independent of $\mathbf{X}$, if given $\mathbf{Y}$. We aim to apply this probabilistic model to understand and optimize the quality of images to improve expected health outcomes for patients.

![Probabilistic graphical model of CT data acquisition and image formation.](figures/tunableNN/probModel.png)

#### Variance and Bias in Reconstructed Images
